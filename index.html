<!DOCTYPE html>
<html>
<head>
	<section id="header">
		<header>
		<h1>How does music evolve over time?</h1>
		<p> Justin Duong - Jared Leong - Kevin Mouayang</p>
		</header>
		<p>
			The following visualization was created using the
			<a href="https://www.kaggle.com/theoverman/the-spotify-hit-predictor-dataset">
				Spotify Hit Predictor Dataset from Kaggle</a>.
		</p>
		<p>
			All of the songs used to produce the visualization were songs that were
			featured at least once on Billboards Hot 100 list. Hence, making the song a "hit"
		</p>
		<p>
			The audio features (valence, energy etc.) come from the
			<a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/">
				 Spotify Web API</a>
		</p>
	</section>
	<p>

	</p>
	<footer>
		<a href="#banner" class="button style2 scrolly">Audio feature definitions</a>
	</footer>

	<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
	<title>Spotify Data Radar Chart</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	<!-- Google fonts -->
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Raleway' rel='stylesheet' type='text/css'>

	<!-- D3.js -->
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.6/d3.min.js" charset="utf-8"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3-legend/1.3.0/d3-legend.js" charset="utf-8"></script>

	<script src='https://d3js.org/d3.v4.min.js'></script>
	
	<style>
		body {
			font-family: 'Open Sans', sans-serif;
			font-size: 15px;
			font-weight: 500;
			fill: #ffffff;
			text-align: center;
			<!--text-shadow: 0 1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff, 0 -1px 0 #fff;-->
			cursor: default;
		}

		.tooltip {
			fill: #242424;
			font-size: 20px;
			font-weight: 600;
		}
	</style>
</head>
<body>
<div class="radarChart"></div>
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.poptrox.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>
<script src="radarChart.js"></script>
<script>
	//////////////////////////////////////////////////////////////
	//////////////////////// Set-Up //////////////////////////////
	//////////////////////////////////////////////////////////////

	var margin = {top: 100, right: 200, bottom: 100, left: 200},
			legendPosition = {x: 10, y: 500},
			width = Math.min(700, window.innerWidth - 10)-200,
			//width = Math.min(1000, window.innerWidth) - margin.left - margin.right,
			height = Math.min(width, window.innerHeight-200);

	//////////////////////////////////////////////////////////////
	//////////////////// Draw the Chart //////////////////////////
	//////////////////////////////////////////////////////////////

	var color = d3.scale.ordinal()
			.range(["#1DB954","#CC333F","#00A0B0","#b000a0","#3CF3CC","#f5aa42"]);

	var radarChartOptions = {
		w: width,
		h: height,
		margin: margin,
		legendPosition: legendPosition,
		maxValue: 0.5,
		wrapWidth: 60,
		levels: 5,
		roundStrokes: true,
		color: color,
		axisName:"categories",
		areaName:"year",
		value: "value"
	};

	//Load the data and Call function to draw the Radar chart
	d3.json("spotifyHits.json", function(error, data){
		RadarChart(".radarChart", data, radarChartOptions);
	});
</script>
<script>
        const width = window.innerWidth,
            height = window.innerHeight,
            maxRadius = (Math.min(width, height) / 2) - 5;

        const formatNumber = d3.format(',d');

        const x = d3.scaleLinear()
            .range([0, 2 * Math.PI])
            .clamp(true);

        const y = d3.scaleSqrt()
            .range([maxRadius*.1, maxRadius]);

        const color = d3.scaleOrdinal(d3.schemeCategory20);

        const partition = d3.partition();

        const arc = d3.arc()
            .startAngle(d => x(d.x0))
            .endAngle(d => x(d.x1))
            .innerRadius(d => Math.max(0, y(d.y0)))
            .outerRadius(d => Math.max(0, y(d.y1)));

        const middleArcLine = d => {
            const halfPi = Math.PI/2;
            const angles = [x(d.x0) - halfPi, x(d.x1) - halfPi];
            const r = Math.max(0, (y(d.y0) + y(d.y1)) / 2);

            const middleAngle = (angles[1] + angles[0]) / 2;
            const invertDirection = middleAngle > 0 && middleAngle < Math.PI; // On lower quadrants write text ccw
            if (invertDirection) { angles.reverse(); }

            const path = d3.path();
            path.arc(0, 0, r, angles[0], angles[1], invertDirection);
            return path.toString();
        };

        const textFits = d => {
            const CHAR_SPACE = 6;

            const deltaAngle = x(d.x1) - x(d.x0);
            const r = Math.max(0, (y(d.y0) + y(d.y1)) / 2);
            const perimeter = r * deltaAngle;

            return d.data.name.length * CHAR_SPACE < perimeter;
        };

        const svg = d3.select('body').append('svg')
            .style('width', '100vw')
            .style('height', '100vh')
            .attr('viewBox', `${-width / 2} ${-height / 2} ${width} ${height}`)
            .on('click', () => focusOn()); // Reset zoom on canvas click

        d3.json('https://gist.githubusercontent.com/mbostock/4348373/raw/85f18ac90409caa5529b32156aa6e71cf985263f/flare.json', (error, root) => {
            if (error) throw error;

            root = d3.hierarchy(root);
            root.sum(d => d.size);

            const slice = svg.selectAll('g.slice')
                .data(partition(root).descendants());

            slice.exit().remove();

            const newSlice = slice.enter()
                .append('g').attr('class', 'slice')
                .on('click', d => {
                    d3.event.stopPropagation();
                    focusOn(d);
                });

            newSlice.append('title')
                .text(d => d.data.name + '\n' + formatNumber(d.value));

            newSlice.append('path')
                .attr('class', 'main-arc')
                .style('fill', d => color((d.children ? d : d.parent).data.name))
                .attr('d', arc);

            newSlice.append('path')
                .attr('class', 'hidden-arc')
                .attr('id', (_, i) => `hiddenArc${i}`)
                .attr('d', middleArcLine);

            const text = newSlice.append('text')
                .attr('display', d => textFits(d) ? null : 'none');

            // Add white contour
            text.append('textPath')
                .attr('startOffset','50%')
                .attr('xlink:href', (_, i) => `#hiddenArc${i}` )
                .text(d => d.data.name)
                .style('fill', 'none')
                .style('stroke', '#fff')
                .style('stroke-width', 5)
                .style('stroke-linejoin', 'round');

            text.append('textPath')
                .attr('startOffset','50%')
                .attr('xlink:href', (_, i) => `#hiddenArc${i}` )
                .text(d => d.data.name);
        });

        function focusOn(d = { x0: 0, x1: 1, y0: 0, y1: 1 }) {
            // Reset to top-level if no data point specified

            const transition = svg.transition()
                .duration(750)
                .tween('scale', () => {
                    const xd = d3.interpolate(x.domain(), [d.x0, d.x1]),
                        yd = d3.interpolate(y.domain(), [d.y0, 1]);
                    return t => { x.domain(xd(t)); y.domain(yd(t)); };
                });

            transition.selectAll('path.main-arc')
                .attrTween('d', d => () => arc(d));

            transition.selectAll('path.hidden-arc')
                .attrTween('d', d => () => middleArcLine(d));

            transition.selectAll('text')
                .attrTween('display', d => () => textFits(d) ? null : 'none');

            moveStackToFront(d);

            //

            function moveStackToFront(elD) {
                svg.selectAll('.slice').filter(d => d === elD)
                    .each(function(d) {
                        this.parentNode.appendChild(this);
                        if (d.parent) { moveStackToFront(d.parent); }
                    })
            }
        }
    </script>
<section id="banner">
	<p>
		<i>Danceability</i> - Danceability describes how suitable
		a track is for dancing based on a combination of
		musical elements including tempo, rhythm stability,
		beat strength, and overall regularity. A value of 0.0
		is least danceable and 1.0 is most danceable.
	</p>
	<p>
		<i>Valence</i> - A measure from 0.0 to 1.0 describing the
		musical positiveness conveyed by a track. Tracks with
		high valence sound more positive (e.g. happy, cheerful, euphoric),
		while tracks with low valence sound more negative
		(e.g. sad, depressed, angry).
	</p>
	<p>
		<i>Energy</i> - Energy is a measure from 0.0 to 1.0 and
		represents a perceptual measure of intensity and activity.
		Typically, energetic tracks feel fast, loud, and noisy. For
		example, death metal has high energy, while a Bach prelude
		scores low on the scale. Perceptual features contributing to
		this attribute include dynamic range, perceived loudness, timbre,
		onset rate, and general entropy.
	</p>
	<p>
		<i>Liveness</i> - Detects the presence of an audience in the recording.
		Higher liveness values represent an increased probability that
		the track was performed live. A value above 0.8 provides strong
		likelihood that the track is live.
	</p>
	<p>
		<i>Instrumentalness</i> - Predicts whether a track contains no vocals.
		“Ooh” and “aah” sounds are treated as instrumental in this context.
		Rap or spoken word tracks are clearly “vocal”. The closer the
		instrumentalness value is to 1.0, the greater likelihood the track
		contains no vocal content. Values above 0.5 are intended to represent
		instrumental tracks, but confidence is higher as the value approaches 1.0.
	</p>
	<p>
		<i>Acousticness</i> - A confidence measure from 0.0 to 1.0 of whether
		the track is acoustic. 1.0 represents high confidence the track
		is acoustic.
	</p>
	<p>
		<i>Speechiness</i> - Speechiness detects the presence of spoken words
		in a track. The more exclusively speech-like the recording
		(e.g. talk show, audio book, poetry), the closer to 1.0 the attribute
		value. Values above 0.66 describe tracks that are probably made
		entirely of spoken words. Values between 0.33 and 0.66 describe
		tracks that may contain both music and speech, either in sections
		or layered, including such cases as rap music. Values below 0.33
		most likely represent music and other non-speech-like tracks.
	</p>

</section>
<article style="background-color: #35b88f;" class="container box style3">
	<header>
		<h2>Write-Up</h2>
		<p style="font-style: italic;">1) A rationale for your design decisions.
			How did you choose your particular visual encodings and
			interaction techniques? What alternatives did you consider
			and how did you arrive at your ultimate choices?</p>
		<p>Our question was based around if the
			traits of “hit” songs changed over time so we
			wanted a visualization that could emphasize the
			differences in the values of these traits
			throughout difference decades of music. One
			of the alternatives we considered was a heatmap,
			but we felt that a heatmap wasn’t interactive
			enough and that something like a radar chart was
			more applicable to our question and dataset.</p>
		<p style="font-style: italic;">2) An overview of
			your development process. Describe how the work
			was split among the team members. Include a
			commentary on the development process, including
			answers to the following questions: Roughly how
			much time did you spend developing your application
			(in people-hours)? What aspects took the most time?</p>
		<p>We started out by brainstorming ideas for what kind
			of dataset we could use based on our own interests
			(music?). After finding a dataset, we familiarized
			ourselves with possible visualizations we could use
			and looked at their implementations using D3. Once we
			settled on the radar chart, we split the work between
			implementing the visualization and putting things like
			text on the page, debugging, and transforming the dataset
			using Trifacta. The most time-consuming part was figuring
			out which dataset and visualization to use. We spent
			around 30 hours in total to complete the assignment from
			start to finish.</p>
	</header>
</article>
	<section id="footer">

	</section>
</body>
</html>
